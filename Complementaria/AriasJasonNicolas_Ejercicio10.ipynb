{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Ca47pvrzRXMy",
      "metadata": {
        "id": "Ca47pvrzRXMy"
      },
      "source": [
        "## Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6f64d18c-e6fe-4328-a15d-9344842eabd1",
      "metadata": {
        "id": "6f64d18c-e6fe-4328-a15d-9344842eabd1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sympy as sym"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iKUX0yLUdO7o",
      "metadata": {
        "id": "iKUX0yLUdO7o"
      },
      "source": [
        "## Problema\n",
        "\n",
        "Considere los siguientes puntos\n",
        "\n",
        "```\n",
        "x = [1., 2., 3., 4., 5.]\n",
        "y = [1.20,  0.31, 3.92, 3.78, 4.47]\n",
        "```\n",
        "\n",
        "Basados en el metodo de los mínimos cuadrados, se puede ajustar una parábola a los puntos\n",
        "\n",
        "$$ y = a + bx + cx^2$$\n",
        "\n",
        "En este problema vamos a considerar la regresión, por medio de la gradiente descendiente,\n",
        "\n",
        "a) El método de minimos cuadrados por gradiente descediente considera minimizar la distancia cuadratica entre los puntos. Implemente una función que retorne la siguiente función de costo, la cual se espera que sea mínima en el punto optimo de la regresión. \n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\sum_{i=1}^N(y_i - (a + bx_i + cx_i^2))^2\n",
        "$$\n",
        "\n",
        "Donde $a$, $b$, y $c$ son los valores a optimizar. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "14dX13PHYbMj",
      "metadata": {
        "id": "14dX13PHYbMj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9960.6318"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([1., 2., 3., 4., 5.])\n",
        "y = np.array([1.20,  0.31, 3.92, 3.78, 4.47])\n",
        "\n",
        "def loss(x, y, a, b, c):\n",
        "  loss = 0\n",
        "  for i in range(len(x)):\n",
        "    loss += (y[i] - (a+b*x[i]+c*x[i]**2))**2\n",
        "  return loss\n",
        "\n",
        "# codigo para verificar su respuesta\n",
        "loss(x, y, -1, 2, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JNhmTJ2BeSvQ",
      "metadata": {
        "id": "JNhmTJ2BeSvQ"
      },
      "source": [
        "b) Para el algoritmo de gradiente descendiente, se requiere la gradiente de la función de costo. Obtenga la gradiente de la función de costo de forma analitica,\n",
        "\n",
        "Su expresión aquí,\n",
        "\n",
        "$$\n",
        "\\vec{\\nabla} \\mathcal{L} = \\Big(\\frac{\\partial  \\mathcal{L}}{\\partial a} , \\quad \\frac{\\partial  \\mathcal{L}}{\\partial b}, \\quad \\frac{\\partial  \\mathcal{L}}{\\partial c} \\Big) =\\Big(\\ , \\ , \\ \\Big)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dW34NY1YfeMH",
      "metadata": {
        "id": "dW34NY1YfeMH"
      },
      "source": [
        "c) Implemente una función que retorne la gradiente de la función de costo obtenida en el númeral anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d84a93fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def symbolic_loss_function():\n",
        "  x, y, a, b, c = sym.symbols('x y a b c')\n",
        "  loss = (y - (a+b*x+c*x**2))**2\n",
        "  return loss\n",
        "\n",
        "def symbolic_loss_function_derivative():\n",
        "  x_s, y_s, a_s, b_s, c_s = sym.symbols('x y a b c')\n",
        "  loss = (y_s - (a_s+b_s*x_s+c_s*x_s**2))**2\n",
        "  loss_a = sym.lambdify((x_s, y_s, a_s, b_s, c_s), sym.diff(loss, a_s))\n",
        "  loss_b = sym.lambdify((x_s, y_s, a_s, b_s, c_s), sym.diff(loss, b_s))\n",
        "  loss_c = sym.lambdify((x_s, y_s, a_s, b_s, c_s), sym.diff(loss, c_s))\n",
        "  return loss_a, loss_b, loss_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "p5qXQvevYbQE",
      "metadata": {
        "id": "p5qXQvevYbQE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 22.64,  87.9 , 370.1 ])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def grad_loss(x, y, a, b, c):\n",
        "    grad = np.zeros(3)\n",
        "    da, db, dc = symbolic_loss_function_derivative()\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        grad[0] += da(x[i], y[i], a, b, c)\n",
        "        grad[1] += db(x[i], y[i], a, b, c)\n",
        "        grad[2] += dc(x[i], y[i], a, b, c)\n",
        "\n",
        "    return np.array(grad)\n",
        "\n",
        "\n",
        "# codigo para verificar su respuesta\n",
        "grad_loss(x, y, -1, 2, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PkhobT8dgB0j",
      "metadata": {
        "id": "PkhobT8dgB0j"
      },
      "source": [
        "d) Implemente el algoritmo de gradiente descendiente, para encontrar los valores optimos de la regresión, use $e = 0.0001$, 10000 iteraciones, e imprima el valor de la función de costo cada 100 iteraciones. \n",
        "\n",
        "El algoritmo de gradiente descendiente nos indica que los puntos optimos de la regresión se van actualizando iterativamente, a partir de la regla, \n",
        "\n",
        "$$\n",
        "  \\vec{x} = \\vec{x} - e\\vec{\\nabla} \\mathcal{L}\n",
        "$$\n",
        "\n",
        "Donde $e$ es el tamaño del paso, y $\\vec{x} = [a, b, c]$. Pueden usar el siguiente notebook como guía [gradiente descendiente](https://github.com/diegour1/CompMetodosComputacionales/blob/main/Notebooks/08%20-%20Gradient_Descent_Neural_Networks.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "rqUAS3V_YbS4",
      "metadata": {
        "id": "rqUAS3V_YbS4"
      },
      "outputs": [],
      "source": [
        "# su codigo aqui\n",
        "def GradientDescent(x, y, a, b, c, e, iter):\n",
        "    for i in range(iter):\n",
        "        grad = grad_loss(x, y, a, b, c)\n",
        "        a = a - e*grad[0]\n",
        "        b = b - e*grad[1]\n",
        "        c = c - e*grad[2]\n",
        "    return a, b, c"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MSmaQFKhiMIJ",
      "metadata": {
        "id": "MSmaQFKhiMIJ"
      },
      "source": [
        "e) Imprima los valores finales de la regresión y de la función de costo para los valores optimos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "wnh5PS3YYbbB",
      "metadata": {
        "id": "wnh5PS3YYbbB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.461287960587385e+128 5.933770187993464e+128 2.5721860397610182e+129\n"
          ]
        }
      ],
      "source": [
        "# su codigo aqui\n",
        "a_opt, b_opt, c_opt = GradientDescent(x, y, 1, 1, 1, 0.01, 100)\n",
        "print(a_opt, b_opt, c_opt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HCM7ZTbll6Iv",
      "metadata": {
        "id": "HCM7ZTbll6Iv"
      },
      "source": [
        "f) Ahora obtenga los valores optimos de la regresión, usando el método matricial. Puede usar el siguiente notebook como guía, [Minimos cuadrados](https://github.com/diegour1/CompMetodosComputacionales/blob/main/Notebooks/05%20-%20fit_lineal_y_solucion_sistema_de_ecuaciones.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qFy15KA0meiL",
      "metadata": {
        "id": "qFy15KA0meiL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 0.] 51.1718\n"
          ]
        }
      ],
      "source": [
        "param = np.zeros(3)\n",
        "# su codigo aqui\n",
        "\n",
        "print(param, loss(x, y, *tuple(param)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1AvYxum4nGfi",
      "metadata": {
        "id": "1AvYxum4nGfi"
      },
      "source": [
        "g) ¿La función de costo con el método de regresión matricial es mayor o menor que con el método de gradiente descediente? ¿Sí, No, por qué?\n",
        "\n",
        "Su comentario aqui\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3aZ-I4smm96",
      "metadata": {
        "id": "b3aZ-I4smm96"
      },
      "source": [
        "h) Realice una gráfica mostrando los puntos $x$ y $y$, ademas de las gráficas de las funciones obtenidas con el método de gradiente descendiente y con el método de regresión matricial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4pWICAMEebbs",
      "metadata": {
        "id": "4pWICAMEebbs"
      },
      "outputs": [],
      "source": [
        "# su codigo aqui\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
